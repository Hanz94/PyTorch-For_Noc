{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f81a97d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccd79ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir, file_index):\n",
    "        self.data_dir = data_dir\n",
    "        self.file_index = file_index\n",
    "        y = np.load(self.data_dir + \"Y/\" + str(self.file_index) + \".npy\", allow_pickle=True)\n",
    "        self.y = torch.from_numpy(y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = np.load(self.data_dir + \"X/\" + str(self.file_index) + \".npy\", allow_pickle=True, mmap_mode='r')\n",
    "        x = torch.from_numpy(x)\n",
    "        return [x[index], self.y[index]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27ae120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = MyDataset(\"/home/hansika/gem5/gem5/scripts/numpy_data/64_nodes/\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "036d4498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399168"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6e464fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samll set of data for trial run\n",
    "data_set,data_set_1 = torch.utils.data.random_split(data_set, [100, 399068]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad7fda6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-858f4186a916>:12: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448216815/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  x = torch.from_numpy(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[ 81,   1,   1,   1,   2,  51, 268,  17, 187,   1,   1,   1,   2, 132,\n",
       "          100,  57,  41,  23,  50,   1,   1,   1,   2, 190,  49,   1,   1,   1,\n",
       "            2, 234, 177,   1,   1,   1,   2, 358,  66, 152,   1,   1,   1,   2,\n",
       "          146,   1,   1,   1,   2,  37, 154, 216,   1,   1,   1,   2,   9, 191,\n",
       "            1,   1,   1,   2,  38,  30,   1,   1,   1,   2, 177,  38,  15,   1,\n",
       "            1,   1,   2,   8,  44,   1,   1,   1,   2, 211,  45, 182, 317,  23,\n",
       "           19,  70,   1,   1,   1,   2,  55, 231,  20, 206, 234,  62,  14,   2,\n",
       "          128, 355, 140,   1,   1,   1,   1,   1,  89, 107, 134, 117, 123, 180,\n",
       "           59, 161, 199,  41,   1,   1,   1,   2,  77, 205,  16,  20,  64,  78,\n",
       "          210,  51,  58,  21, 512,  32,  21,   5,   3,   1,   1,   1,   2,  37,\n",
       "          289,   1,   1,   1,   2, 105,  23,   7, 303,   1,   1,   1,   2, 218,\n",
       "          253,  14, 132,  32, 129,   1,  25,   1,   1,   1,   2, 150,   3, 177,\n",
       "           18,  19,   1,   1,   1,   2,  37, 264,  80,  13, 124,   1,   1,   1,\n",
       "            1,   1,  57, 368, 145,  36,  62,   1,   1,   1,   2,  66,   1,   1,\n",
       "            1,   2,   9,  83, 142, 270,   1,   1,   1,   2,  34,   1,   1,   1,\n",
       "            2,   7,   1,   1,   1,   2, 217, 110,   1,   1,   1,   2,  34,  60,\n",
       "          200, 174,   1,   1,   1,   2,  81,   1,   1,   1,   2,  74,   1,   1,\n",
       "            1,   2,  60,  15,  91,  79,  31,  53,  46, 116,  25,   1,   1,   1,\n",
       "            2, 173,   8,  26,  26,  76,  71,   4,  74, 115, 212,  40,   1,   1,\n",
       "            1,   2,  66,  22,  93,   1,   1,   1,   2,  37,  44,   1,   1,   1,\n",
       "            2, 306,   1,   1,   1,   2,  34,  67,   1,   1,   1,   2,  47,  55,\n",
       "          319,   1,   1,   1,   2,  60,   2, 146,  99,  35,   1,   1,   1,   2,\n",
       "          143, 192,  28,   1,   1,   1,   1,   1, 185,  98,  66,  20,  42,   6,\n",
       "            7,  64,  47,  84,  11,   7, 127, 145, 315,   1,   1,   1,   2,   1,\n",
       "            1,   1,   1,   2,  33,  10, 215,   1,   1,   1,   2,  12, 207, 349,\n",
       "            9,  28, 172,   9,  15,   1,   1,   1,   2,   7,   1,   1,   1,   2,\n",
       "           19, 457,   1,   1,   1,   2, 106,  63,  17,   1,   1,   1,   2,  13,\n",
       "           59,   1,   1,   1,   2, 110, 194, 209,   1,   1,   1,   2,  57,  29,\n",
       "           40,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [278,  97, 329, 154,  64,  45,   1,   1,   1,   2,  12, 318,  90,   1,\n",
       "            1,   1,   2,  12,   2,   2,   2,   2,  62, 267,   1,   1,   1,   2,\n",
       "           84,  12, 168,   1,  22,   1,   2,   3,   2,  57,   1,   1,   1,   3,\n",
       "          103,  94,  32, 104,  83,   1,   1,   1,   1,   1,  59,  85,   1,   1,\n",
       "            1,   2,  14,  16, 654,  29,  12,   1,   1,   1,   2,  24,  61,  20,\n",
       "            9,   1,   1,   1,   2,  54,  49,  96, 305, 100, 337,  69,   1,   1,\n",
       "            1,   2,  24,  32,  53,  20,  93,  52,   1,   1,   2,   2,  67,  85,\n",
       "           98, 202, 353, 163,   1,   1,   1,   2,  22, 108,  63,  14,   1,   1,\n",
       "            1,   2,  55,   1,   1,   1,   2,   5, 153,  57,   1,   1,   1,   2,\n",
       "           19,   2,   1,   1,   1, 145,  85,  35,   8,   2,   2,   1,   1,   2,\n",
       "           42,  27,  13,  12, 194, 171,   5,   2,   2,   2,  57, 191, 139, 136,\n",
       "            4,   1,   1,   1,  47,   1,   1,   1,   2,  28,   2,   2,   1,   1,\n",
       "          201,  36,   2,   1,   1,   1,  31,   3,  58,  86, 199,   1,   1,   1,\n",
       "            1,   1,   1,   1,   1,   1,  82,   1,   1,   1,   2,  56,  49,   1,\n",
       "            1,   1,   2,  22,   2,   1,   1,   1,   3,   1,   1,   1,   2,   1,\n",
       "           16, 249,  61,  11,  32,   8,  66, 120, 209,   1,   1,   1,   2,  23,\n",
       "           47, 172,  19,   2,   1,   1,   1,  15,  37,  46,   1,   1,   1,   2,\n",
       "           31,   1,   1,   1,   2,   9,   1,   1,   1,   2,  55,   1,   1,   1,\n",
       "            2,   2,   1,   1,   1,   2, 137,   1,   1,   1,   2,   1,   3,  40,\n",
       "          261, 181,  10, 167,  26, 122, 133,   2,   1,   1,   1,  61,   9,  80,\n",
       "            2,   1,   1,   1, 115,   1,   1,   1,   2, 255,   1,   1,   1,   2,\n",
       "           50, 125, 160, 142,   1,   1,   1,   2, 124,   9, 118, 183, 306,  16,\n",
       "          700,  88,  47,  26,  30,   1,   1,   1,   2, 156,  32,   7,  71,   1,\n",
       "            1,   1,   2,   1, 163,  91,  65,  15,   1,   1,   1,   2,  61,  99,\n",
       "            1,   1,   1,   2, 181, 125,   1,   1,   1,   2,  82,   4,  66,   1,\n",
       "            1,   1,   2, 128,   1,   1,   2,   2,  18,  56,   1,   1,   1,   2,\n",
       "           27,  26,  90, 142, 138,   1,   1,   1,   2,  35,  74,   1,   1,   1,\n",
       "            2, 198,  94,  51,   1,   1,   1,   2,   1, 153, 134, 122, 141,  40,\n",
       "           89,  95,   1,   1,   1,   1,   1, 110,  26,  70,   1,   1,   1,   2,\n",
       "           58,  47,   1,   1,   1,   2, 115,   8,   1,   1,   1,   2,  86,   1,\n",
       "            1,   1,   2,  20, 288,   1,   1,   1,   2,  27,  68, 119, 141,  18,\n",
       "           93,   4,   1,   1,   1,   2,  55,  72,   1,   1,   1,   2,   1, 107,\n",
       "           46,  88]]),\n",
       " tensor(0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b93a83b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full dataset from multiple files\n",
    "\n",
    "list_of_dataset = []\n",
    "number_of_files = 41\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "for i in range(number_of_files):\n",
    "    list_of_dataset.append(MyDataset(\"/home/hansika/gem5/gem5/scripts/numpy_data/64_nodes/\",i))\n",
    "\n",
    "full_dataset = ConcatDataset(list_of_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23f69a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  7,  50,  52,   1,   1,   1,   2,  31,  94,  60,   1,   1,   1,   2,\n",
       "            4,   1,   1,   1,   2, 176, 156,  29,  44, 190,  21,   1,   1,   1,\n",
       "            2,  10,   1,   1,   1,   2,  58,  57,  35,  24,   1,   1,   1,   2,\n",
       "           27,   1,   1,   1,   2,  69,   1,   1,   1,   2,   3,   1,   1,   1,\n",
       "            2,  22,  42,   1,   1,   1,   2,  68,  77,   8,   1,   1,   1,   2,\n",
       "          172,   1,   1,   1,   2,  60,   1,   1,   1,   2,  35,   1,   1,   1,\n",
       "            2,  19,   1,   1,   1,   2, 140,  78, 151, 333, 102,   1,   1,   1,\n",
       "            1,   1,  66,  22,   1,   1,   1,   2,  38,  34, 123,   1,   1,   1,\n",
       "            2,  26, 196,  76,   1,   1,   1,   2,   1,   1,   1,   1,   1,  68,\n",
       "            1,   1,   1,   2,  54,   1,   1,   1,   2,  80,   1,   1,   1,   2,\n",
       "           48,   1,   1,   1,   2,   1,   7, 127,  64,   1,   1,   1,   2, 134,\n",
       "            1,   1,   1,   2,  46,   1,   1,   1,   2,  34, 140,   1,   1,   1,\n",
       "            2,  19,  92,   3, 127,  65, 180,   1,   1,   1,   2,   6,  90,   1,\n",
       "            1,   1,   2,  24, 343,  90,   1,   1,   1,   2,  28, 160, 209, 132,\n",
       "          140,  15,   1,   1,   1,   2, 144,   1,   1,   1,   1,   1,  39,  74,\n",
       "           65,  28, 209, 126, 266,   3,  92, 196,   1,   1,   1,   2, 137,   1,\n",
       "            1,   1,   2,  30,  68,  94,   1,   1,   1,   2,  87,  55, 112, 263,\n",
       "           25, 168,  89,   1,   1,   1,   2, 242, 241,   1,   1,   1,   2,  33,\n",
       "            1,   1,   1,   2,   4,  38,   7,   1,   1,   1,   2,  29,   1,   1,\n",
       "            1,   2,  28,   1,   1,   1,   2, 179,  81,  31,  10,  11,   1,   1,\n",
       "            1,   3,  93,  66,   1,   1,   1,   2,  98,   1,   1,   1,   2,  51,\n",
       "            1,   1,   1,   2, 143,   1,   1,   1,   2,  81,  42,  27,   1,   1,\n",
       "            1,   2,  55,  94,   1,   1,   1,   2, 179,   1,   1,   1,   2,   4,\n",
       "            1,   1,   1,   2, 174,   1,   1,   1,   2,  57, 194, 248,  72,   1,\n",
       "            1,   1,   2,  84,  71, 206,  38, 201,   1,   1,   1,   2,  46, 321,\n",
       "           59,  28,   1,   1,   1,   2,   9, 157,  66,   2,   1,   1,   1,   2,\n",
       "          191,  41,   1,   1,   1,   1,   1,  96,   1,   1,   1,   2, 186,   1,\n",
       "            1,   1,   2, 204,  96,   1,   1,   1,   2,  58, 679, 153,   1,   1,\n",
       "            1,   2, 309,  87,   1,   1,   1,   2,  55,   1,   1,   1,   2, 163,\n",
       "           20, 144,   1,   1,   1,   1,   1,   1,   1,   1,   1,  33,  44,  54,\n",
       "          112,   2, 194,  61,  13,   1,   1,   1,   2,  76,  68,   1,   1,   1,\n",
       "            2,  82,   3, 423, 156,  69, 136, 106,  23,  17,   1,   1,   1,   2,\n",
       "           16,  66],\n",
       "         [ 20, 192,   2,   1,   1,   1,  10, 230, 315,  66,   3,   2,   1,   1,\n",
       "           25,  10, 151, 250,  10, 298,   1,   1,   1,   2,   5, 426,  19,   1,\n",
       "            1,   1,   2,   5,   1,   1,   1,   2, 209,   1,   1,   1,   2,  40,\n",
       "           40,  38, 154,  82,  53,  41, 126,  76, 346,  49,   2,   1,   1,   1,\n",
       "          203,   1,   1,   1,   2,   4, 498,   1,   1,   2,   2, 201,  45,   1,\n",
       "            1,   1,   2,  84,   1,   1,   1,   2, 195,   1,   1,   1,   2,  11,\n",
       "            1,   1,   1,   2, 125,  97,   1,   2,   1,   1,  44, 371,   1,   1,\n",
       "            1,   2,  17,  14,  74, 196,   1,   1,   2,   1, 146,  78,   1,   1,\n",
       "            1,   2,  20,  82, 212,  19,  65,   1,   1,   1,   2, 142,   1,   1,\n",
       "            1,   2,  17, 100,   1,   1,   1,   2,  58, 904,  82,  54,  60,  34,\n",
       "            1,   1,   1,   2,   1, 214, 515, 166, 165,   1,   1,   1,   2, 132,\n",
       "            1,   1,   1,   2,  24,  18,  55,   1,   1,   1,   2,  83,   1,   1,\n",
       "            1,   2,  54,  42,  22,  15,   3, 105,   3, 253,   2,   1,   1,   1,\n",
       "          250,  92,   3,   1,   1,   1,   2,   1,   3,   1,   1,   1,   1,   1,\n",
       "          229,  39,   1,   1,   1,   2,   1, 172,  36,   1,   1,   1,   2,  10,\n",
       "           85,   1,   1,   1,   2,  35,  55,   3,  68,   2,   2,   2,   1,   1,\n",
       "            2,  11,  27,   8, 240,  91,   4, 416,  25,   1,   1,   1,   2, 221,\n",
       "          109,   1,   1,   1,   2,  94,   2,   2,   2,   2,  26,  33,  22, 110,\n",
       "           57,   1,   1,   1,   2, 400, 124,  38, 291, 169,   1,   1,   1,   2,\n",
       "           39, 171,  29,  45,   1,   1,   1,   2, 343,  21,   1,   1,   1,   2,\n",
       "           18,   1,   1,   1,   2,  10, 153,   4,   2,   2,   2,  49,   1,   1,\n",
       "            1,   2,  10,  82, 226,   1,   1,   1,   2,  34,  22,  40,   1,   1,\n",
       "            1,   2, 266, 348,  55,  12,  13,  26,  89, 176,   1,  33,   1,   1,\n",
       "            1,   2, 306, 193,   9,  17,  17, 113,   1,   1,   1,   2,  28,  33,\n",
       "          452,  66,   1,   1,   1,   2,  31,  97,  51, 105,   1,   1,   1,   2,\n",
       "           34,   1,   1,   1,   2,  57,  62,   2,   2,   2,   2,  97, 322,   1,\n",
       "            1,   1,   2, 118,  23,   1,   1,   1,   2,   3, 126,   1,   1,   1,\n",
       "            2,  22, 317, 232,   2,   1,   1,   1,  82,  47,   1,   1,   1,   2,\n",
       "           93,  52,  65,   1,   1,   1,   2,  57,   1,   1,   1,   2, 102,  85,\n",
       "          151, 125, 131,   6,  59, 144,  75,   2,   1,   1,   1,  26,   1,   1,\n",
       "            1,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0]]),\n",
       " tensor(0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "171f0994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16257024"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64790e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hansika/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448216815/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# W1, W2, K1, K2 are hyper parameters that eventually needed training\n",
    "W1 = 30\n",
    "W2 = 10\n",
    "K1= 2000\n",
    "K2 = 1000\n",
    "\n",
    "#dummy data to try the NN ( 2 arrays of size 450)\n",
    "# dummy = torch.randn(2,450).view(-1,1,2,450)\n",
    "\n",
    "# represents the whole CNN\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.conv1 = nn.Conv2d(1, K1, (2,W1), stride=(2, 1))\n",
    "        self.pool1 = nn.MaxPool2d((1, 5), stride=(1, 1))\n",
    "        self.conv2 = nn.Conv2d(K1, K2, (1,W2), stride=(1, 1))\n",
    "        self.pool2 = nn.MaxPool2d((1, 5), stride=(1, 1))\n",
    "        \n",
    "        self.fc1 = nn.Linear(1000*404, 3000) # need to automate arriving at this number (1000*254)\n",
    "        self.fc2 = nn.Linear(3000, 800) \n",
    "        self.fc3 = nn.Linear(800,100)\n",
    "        self.fc4 = nn.Linear(100,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        \n",
    "        x = x.view(-1, 1000*404)    \n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "# ------------------- Training the CNN ------------------------------------- ##\n",
    "# For now this code is only to show the structure, I need to add data preparation and modify code accordingly.\n",
    "\n",
    "net = Net()\n",
    "\n",
    "isTraining = True\n",
    "if isTraining:\n",
    "   \n",
    "    BATCH_SIZE = 10\n",
    "    EPOCHS = 1\n",
    "    \n",
    "    trainset = torch.utils.data.DataLoader(data_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # learning rate of the adam optimizer should be a hyperparameter\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        for data in trainset:\n",
    "            X, y = data \n",
    "            net.zero_grad()  \n",
    "            X = X.type(torch.FloatTensor)\n",
    "            output = net(X.view(-1,1,2,450))\n",
    "            loss = nn.CrossEntropyLoss()\n",
    "            loss = loss(output, y)\n",
    "            loss.backward() \n",
    "            optimizer.step()  \n",
    "        print(loss)  \n",
    "\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testset:\n",
    "            X, y = data\n",
    "            output = net(X.view(-1,784))\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "    print(\"Accuracy: \", round(correct/total, 3))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
