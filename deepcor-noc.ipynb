{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f81a97d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e00f34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"/home/hansika/gem5/gem5/scripts/numpy_data_reduced/64_nodes/X/2.npy\",  allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca51c86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size/900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "249b1ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155970"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(data == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccd79ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir, file_index):\n",
    "        self.data_dir = data_dir\n",
    "        self.file_index = file_index\n",
    "        y = np.load(self.data_dir + \"Y/\" + str(self.file_index) + \".npy\", allow_pickle=True)\n",
    "        self.y = torch.from_numpy(y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = np.load(self.data_dir + \"X/\" + str(self.file_index) + \".npy\", allow_pickle=True, mmap_mode='r')\n",
    "        x = torch.from_numpy(x)\n",
    "        return [x[index], self.y[index]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27ae120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = MyDataset(\"/home/hansika/gem5/gem5/scripts/numpy_data_reduced/64_nodes/\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "036d4498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d6e464fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set,data_set_1 = torch.utils.data.random_split(data_set, [100, 399068])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ad7fda6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[160,   8,   1,   1,   1,   2,  25,  47,  24,  44,  11,  13,  12,  19,\n",
       "           10,   1,   1,   1,   2,  67,  65, 121,  52, 187,  81, 126,  16,  27,\n",
       "            1,   1,   1,   2,  21, 112, 238, 127,   1,   1,   1,   2,   1,   1,\n",
       "            1,   1,   1,  48,  29,   5,   1,   1,   1,   2,  11,   1,   1,   1,\n",
       "            2,   5,  60, 260, 100,   1,   1,   1,   2, 106, 113,  88,   1,   1,\n",
       "            1,   2, 216,  23,   1,   1,   1,   2, 490,  64,  55,   1,   1,   1,\n",
       "            2,  38,   1,   1,   1,   2,  87,  60,   1,   1,   1,   2, 163, 144,\n",
       "           45,   1,   1,   1,   2, 147,  59, 212,  43,   1,   1,   1,   2,  77,\n",
       "          131,   1,   1,   1,   2,  91,  13, 146,  73,   1,   1,   1,   2, 227,\n",
       "           95,   1,   1,   1,   2,  37,   1,   1,   1,   2,  47, 169,   1,   1,\n",
       "            1,   2,  74,  44, 478,   1,   1,   1,   1,   2, 162,   1,   1,   1,\n",
       "            2,  73,   1,   1,   1,   2,  47,  56, 106,   1,   1,   1,   2,  22,\n",
       "           63,   1,   1,   1,   2,   7,   1,   1,   1,   2,  93,  59, 439,   3,\n",
       "          235,   1,  37,  38,   1,   1,   1,   1,   1, 204, 293,   1,   1,   1,\n",
       "            2,  70,   1,   1,   1,   1,   1,   1,   1,   1,   1,  82, 443,  18,\n",
       "            1,   1,   1,   2, 469, 117,   1,   1,   1,   2, 360,  12,  33,  10,\n",
       "          109,  13,   1,   1,   1,   2, 143, 321, 164,   1,   1,   1,   2,  11,\n",
       "           48,   1,   1,   1,   2,  59,  27,  19,  63,   1,   1,   1,   2,  69,\n",
       "          188, 106,   1,   1,   1,   2,  12,  80,   1,   1,   1,   2,  83,  79,\n",
       "          118,   1,   1,   1,   2,  37, 263,   1,   1,   1,   2,  71,  95,   1,\n",
       "            1,   1,   2,  71,   2, 158,  67, 264, 215,   1,   1,   1,   2, 219,\n",
       "           31,  76,   1,   1,   1,   1,   1,  59,  14,  33,  66,  96,   1,   1,\n",
       "            1,   2,  63,   1,   1,   1,   2,  70,   1,   1,   1,   2,  12,   1,\n",
       "            1,   1,   2, 112,  53,  57, 158,   1,   1,   1,   2,  86, 183,   1,\n",
       "            1,   1,   2,  71,  45, 116, 129,   1,   1,   1,   2, 211,  30,  25,\n",
       "            1,   1,   1,   2,  67,   8,   1,   1,   1,   1,   1, 193,  73,  38,\n",
       "            1,   1,   1,   2,  85, 132,   1,   1,   1,   2,  49,   1,   1,   1,\n",
       "            2,  62,   1,   1,   1,   2, 250,  22,  33,  93,  60,   1,   1,   1,\n",
       "            2,  84,   1,   1,   1,   2,  85,  20,   1,   1,   1,   2,  61,  14,\n",
       "            1,   1,   1,   2, 126,  16,   7,  23,  27,   1,   1,   1,   2,  11,\n",
       "           53,  19,  22,  32, 621,   1,   1,   1,   1,   1,  38,   1,   1,   1,\n",
       "            2,  41,   1,   1,   1,   2,   1,   1,   1,   1,   2,  17,   2,  56,\n",
       "          113,   1],\n",
       "         [ 42, 120,  82,   1,   1,   1,   2,  79,   1,   1,   1,   2,  48, 165,\n",
       "            2,   1,   1,   1,  82,   1,   1,   2,   1,  20,   1,   1,   1,   2,\n",
       "           57, 153,   1,   1,   2,   1,   6, 423,   1,   1,   1,   2,  42, 111,\n",
       "           13,   1,   1,   1,   2,  96,  88,   1,   1,   1,   2,  21,   9,  50,\n",
       "            7,  15,  72,  47,   1,   1,   1,   2,  36,   1,   2,   2,   2, 230,\n",
       "          473,  72, 198,  36,  36,  15,   1,   1,   1,   2,  15,   5,   1, 129,\n",
       "          104,   1,   1,   1,   2, 282,   1,   1,   1,   2,  23,  34,   1,   1,\n",
       "            2,   2,  74,  67,  90,   6,   1,   1,   1,   2,  42, 212, 144, 247,\n",
       "          123, 168,   2,   2,   2,   2, 103,  34, 130,  74,  90,   1,   1,   1,\n",
       "            1,   1,   1,   1,   1,   1,  96,  67, 265, 156,   1,   1,   1,   2,\n",
       "          137,   4,   3,   1,   1,   1, 113,   1,   1,   1,   2,  91,  23,  23,\n",
       "            1,   1,   1,   2,  47, 103,   2,  39,  15,   1,   1,   1,   2,  46,\n",
       "          193,   1,   1,   1,   2,  44,   1,   1,   1,   2, 195,   2,   1,   1,\n",
       "            1,  26, 106, 393,  52, 106,  91,   1,   1,   1,   2, 146,   1,   1,\n",
       "            1,   2,  96,  42,   9,   2,   1,   1,   1, 155,  40, 101,  15,  61,\n",
       "          122,  63,   1,   1,   1,   2,   1,  71, 167,   1,   1,   1,   2,  55,\n",
       "           33,   5,   1,  20,   2,   1,   1,   1, 232,   1,   1,   1,   2,   2,\n",
       "            3, 197,  38,   1,   1,   1,   2,  44,   1,   1,   1,   2,  83,   1,\n",
       "            1,   1,   2,  11, 116,  30,  80,  42, 243, 163,   1,   1,   1,   2,\n",
       "          104,   2,   2,   2,   1,   8,   3,  16,   1,   1,   1,   2,  60,   8,\n",
       "            1,   1,   1,   2, 250, 203,   1,   1,   1,   2,  56,  35,   2,   2,\n",
       "            1,   1, 191,   2,   2,   2,   2,  63,  54,   2,   1,   1,   1, 157,\n",
       "          104, 119, 131,  59,  19,  26,  24,  95, 101, 110,   9,   1,   1,   1,\n",
       "            2, 153,   1,   1,   1,   2,  40, 129,   1,   1,   1,   2, 262, 215,\n",
       "            2,  71, 167,   1,   1,   1,   2,  35, 143, 270,   1,   1,   1,   2,\n",
       "          121,  20,   1,   1,   1,   3, 248,  56, 490,   1,   1,   1,   3, 118,\n",
       "            2,   1,   1,   1, 215, 568,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "            1,  46,  33, 171, 247,   1,   1,   1,   2,  92,  58,   1,   1,   1,\n",
       "            2, 303,  41,  22, 364,   1,   1,   1,   2,  41,   1,   1,   1,   2,\n",
       "           50,   1,   1,   1,   2,  84,   1,   1,   1,   1,   1, 131,  19,  62,\n",
       "          105,   1,   1,   1,   2,  89, 127,   1,   1,   1,   2,   3, 220,  20,\n",
       "          115,   1,   1,   1,   2, 240, 222,  32,   1,   1,   1,   2,  50,   1,\n",
       "            1,   1]]),\n",
       " tensor(0)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b93a83b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dataset = []\n",
    "number_of_files = 41\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "for i in range(number_of_files):\n",
    "    list_of_dataset.append(MyDataset(\"/home/hansika/gem5/gem5/scripts/numpy_data_reduced/64_nodes/\",i))\n",
    "\n",
    "full_dataset = ConcatDataset(list_of_dataset)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23f69a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 61, 112,  11,   4, 129,  43, 257,   1,   1,   1,   2,  49,  79,   1,\n",
       "            1,   1,   2, 116, 131, 157,  93,  16, 180, 113,  25,   1,   1,   1,\n",
       "            2, 154, 103,   1,   1,   1,   2, 393,  38,   1,   1,   1,   2,  16,\n",
       "          124, 122,   3, 271,  58,  49,  18,  28,   9,   1,   1,   1,   2,   4,\n",
       "            1,   1,   1,   2, 431,   1,   1,   1,   2, 244, 243,  58, 126,  36,\n",
       "           15, 115,  64, 192, 167, 313,  67,   1,   1,   1,   2, 143,   1,   1,\n",
       "            1,   2,  17, 334,   1,   1,   1,   2,  87,   1,   1,   1,   2, 292,\n",
       "          225,  55,   1,   1,   1,   2,  80, 165,  27,   1,   1,   1,   2, 146,\n",
       "          251,  67,  34,  44,  41,   1,   1,   1,   2, 310,  10, 214, 106,   1,\n",
       "            1,   1,   2, 261,   1,   1,   1,   2,  10,   1,   1,   1,   2,  31,\n",
       "            1,   1,   1,   2, 118,   1,   1,   1,   2,  98,  90, 121,   1,   1,\n",
       "            1,   2,  53,   1,   1,   1,   2,  47, 148,   5,   1,   1,   1,   2,\n",
       "          327,  72,  79,  98,  97,  84,   1,   1,   1,   2,  76, 115, 110,   1,\n",
       "            1,   1,   2,   8,   3,   1, 330, 103,   1,   1,   1,   2,  10,   1,\n",
       "            1,   1,   2,  39,  33, 223,   1,   1,   1,   2,  80, 169,  25,  57,\n",
       "           66,   1,   1,   1,   2,  71, 149,   7,  26,  60,  81, 178, 131,   1,\n",
       "            1,   1,   1,   2,  77,  69,   1,   1,   1,   2, 119,   1,   1,   1,\n",
       "            2,  34, 268,  99,   3,   1,   1,   1,   2,  38, 386,   8,   2,   1,\n",
       "            1,   1,   2,  75,   1,   1,   1,   2,  24,  94,  10,   1,   1,   1,\n",
       "            2,   4,  29,   1,   1,   1,   2,  55, 149,  48,  75,   1,   1,   1,\n",
       "            2, 190,  28, 154,  79,  62,  92,   1,   1,   1,   2, 238,  37,  33,\n",
       "          101,   1,   1,   1,   2,  34,  86,  22,  26,   1,   1,   1,   2,   2,\n",
       "            4,   1,   1,   1,   2, 179,  24,  45,  13, 218,   1,   1,   1,   2,\n",
       "           66, 265,   3, 106,   1,   1,   1,   2, 116, 226, 125,  11,   1,   1,\n",
       "            1,   2,   1,   1,   1,   1,   2, 238, 193,  25,   1,   1,   1,   2,\n",
       "          179, 192,  20, 293, 135,   1,   1,   1,   2,  10,  13, 334, 132,  46,\n",
       "          229,  23, 215,  36,  25, 124,  69,   1,   1,   1,   2,  28,   6, 154,\n",
       "            1,   1,   1,   2,  43, 230, 207, 116,   1,   1,   1,   2,  28,   1,\n",
       "            1,   1,   2, 185,  25,  45,  14,  74,  18, 131,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0],\n",
       "         [ 65,   3, 109,  11,   4,  78,  46,   1,   1,   1,   2,   2,  36,   2,\n",
       "            3, 257,   1,   1,   1,   2,  21,  28,  65,   1,   1,   1,   2,   9,\n",
       "            1,   1,   1,   2,  11, 105,  20,   1,   1,   1,   1,   1,   4,  76,\n",
       "           19,   2,   2,   2,   1,   1,  22, 130,   2,   2,   2,   2,   2,  87,\n",
       "           16, 180,  17,  96,  25,   1,   1,   1,   2, 132,  22, 103,   1,   1,\n",
       "            1,   2,  88,  49, 103,  54,  68,  31,   3,  14,  18,   3,   1,   1,\n",
       "            1,   2,  16,  48,  76, 122,   3, 115,  58,  54,   2,   1,   1,   1,\n",
       "           20,  19,  58,  49,  18,  28,   9,   1,   1,   1,   2,   4,   1,   1,\n",
       "            1,   2,  51,  58,   1,   2,   1,   1, 131, 186,   1,   1,   1,   2,\n",
       "          109,  75,   1,   1,   1,   2,  55, 243,  20,  38,  46,   9,   1,   1,\n",
       "            1,   2,  66,   9,   1,   1,   1,   2,  22,  15,  80,   1,   1,   1,\n",
       "            2,  30,  58,   6, 104,   1,   1,   1,   2,  58,  25,   7,   1,   1,\n",
       "            1,   2,  18,   1,   1,   1,   1,   1,  89,  19,  24, 139,   1,   1,\n",
       "            1,   2,  24,  99,   1,  15,  30,  67,   1,   1,   1,   2,  87,  33,\n",
       "            1,   1,   1,   2,  18,   1,   1,   1,   2,  17,  31,   1,  14,   1,\n",
       "            1,   1,   2, 240,   2,   1,   1,   1,   2,   9,   1,   1,   1,   2,\n",
       "           22,   1,   1,   1,   2,  87,   1,   1,   1,   2,  32,   1,   1,   1,\n",
       "            2,   1,  95,  75,   1,   1,   1,   2,  79, 188,  29,   2,   2,   2,\n",
       "            2,   1,  54,   1,   1,   1,   2,  17,  44,  19,  88,  77,   5,  22,\n",
       "            1,   1,   1,   3,   9, 136, 192,   1,   1,   1,   2,  54,  51,   1,\n",
       "            1,   1,   2,  11,  19,   1,   1,   1,   2,   4,   6,  44,  41,   1,\n",
       "            1,   1,   2,  73,   1,   1,   1,   2,  50,   1,   1,   1,   2,  57,\n",
       "           81,   1,   1,   1,   2,  34,  10, 147,   1,   1,   1,   2,   2,  60,\n",
       "           80,  26,   1,   1,   1,   2,  24, 237,   1,   1,   1,   2,  10,   1,\n",
       "            1,   1,   2,  31,   1,   1,   1,   2,  32,  31,  55,   1,   1,   1,\n",
       "            2,  15,  83,  47,   1,   1,   1,   2,  38, 121,   1,   1,   1,   2,\n",
       "           20,  33,   1,   1,   1,   2,  47, 148,   5,   1,   1,   1,   2,  44,\n",
       "            1,   1,   1,   1,   2,  52,   1,   1,   1,   2, 147,   2,   2,   2,\n",
       "            1,  66,  72,   4,  48,   1,   1,   1,   1,  23,  98,  97,  56,  28,\n",
       "            1,   1,   2,   1,  76,  24,   1,   1,   1,   2,  12,   1,   1,   1,\n",
       "            2,  44,   1,   1,   1,   2,  20, 110,   1,   1,   1,   2,   8,   3,\n",
       "            1, 137, 104,   1,   1,   1,   1,   1,  61,  23, 103,   1,   1,   1,\n",
       "            2,  10]]),\n",
       " tensor(1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "171f0994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16128"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f92cc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-858f4186a916>:12: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448216815/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  x = torch.from_numpy(x)\n"
     ]
    }
   ],
   "source": [
    "x,y = data_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bbfe4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "782d5d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 8064, 1: 8064}\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "\n",
    "counter_dict = {0:0, 1:0}\n",
    "\n",
    "for data in full_dataset:\n",
    "    Xs, Ys = data\n",
    "    counter_dict[(int(Ys))] += 1\n",
    "        \n",
    "print(counter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9db7c4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.randn(2,450).view(-1,1,2,450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "780df1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = data_set[100][0].view(-1,1,2,450)\n",
    "dummy = dummy.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ca89221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-858f4186a916>:12: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448216815/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  x = torch.from_numpy(x)\n"
     ]
    }
   ],
   "source": [
    "dummy = data_set[100][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d5eb1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set[100][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64790e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# W1, W2, K1, K2 are hyper parameters that eventually needed training\n",
    "W1 = 30\n",
    "W2 = 10\n",
    "K1= 2000\n",
    "K2 = 1000\n",
    "\n",
    "#dummy data to try the NN ( 2 arrays of size 450)\n",
    "# dummy = torch.randn(2,450).view(-1,1,2,450)\n",
    "\n",
    "# represents the whole CNN\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.conv1 = nn.Conv2d(1, K1, (2,W1), stride=(2, 1))\n",
    "        self.pool1 = nn.MaxPool2d((1, 5), stride=(1, 1))\n",
    "        self.conv2 = nn.Conv2d(K1, K2, (1,W2), stride=(1, 1))\n",
    "        self.pool2 = nn.MaxPool2d((1, 5), stride=(1, 1))\n",
    "        \n",
    "        self.fc1 = nn.Linear(1000*404, 3000) # need to automate arriving at this number (1000*254)\n",
    "        self.fc2 = nn.Linear(3000, 800) \n",
    "        self.fc3 = nn.Linear(800,100)\n",
    "        self.fc4 = nn.Linear(100,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        \n",
    "        x = x.view(-1, 1000*404)    \n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return torch.sigmoid(x)\n",
    "        \n",
    "\n",
    "    \n",
    "## ------------------------------------testing small net--------------------------- ##\n",
    "\n",
    "# net = SmallNet()\n",
    "# print(net)\n",
    "# print(type(dummy))\n",
    "\n",
    "# output = net.forward(dummy.narrow(2,0,2))\n",
    "# output.shape\n",
    "\n",
    "## --------------------------------end testing small net----------------------------- ##\n",
    "\n",
    "## ---------------------------------------- testing net ----------------------------- ##\n",
    "\n",
    "# net = Net()\n",
    "# print(net)\n",
    "\n",
    "# output = net.forward(dummy)\n",
    "# output.shape\n",
    "\n",
    "## -------------------------------end testing net------------------------------------ ##\n",
    "\n",
    "\n",
    "\n",
    "# ------------------- Training the CNN ------------------------------------- ##\n",
    "# For now this code is only to show the structure, I need to add data preparation and modify code accordingly.\n",
    "\n",
    "isTraining = False\n",
    "if isTraining:\n",
    "   \n",
    "    BATCH_SIZE = 10\n",
    "    EPOCHS = 1\n",
    "    \n",
    "    # need to use collected data here\n",
    "    trainset = torch.utils.data.DataLoader(data_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "#     testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=False)\n",
    "\n",
    "    # learning rate of the adam optimizer should be a hyperparameter\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        for data in trainset:\n",
    "            X, y = data \n",
    "            net.zero_grad()  \n",
    "            X = X.type(torch.FloatTensor)\n",
    "            output = net(X.view(-1,1,2,450))\n",
    "            loss = nn.CrossEntropyLoss()\n",
    "            loss = loss(output, y)\n",
    "            loss.backward() \n",
    "            optimizer.step()  \n",
    "        print(loss)  \n",
    "\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testset:\n",
    "            X, y = data\n",
    "            output = net(X.view(-1,784))\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "    print(\"Accuracy: \", round(correct/total, 3))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37a510d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hansika/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448216815/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "dummy = dummy.type(torch.FloatTensor)\n",
    "output = net.forward(dummy.view(-1,1,2,450))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0e3de8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.8159]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce95215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_set[100][1]\n",
    "y = torch.LongTensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15f47b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ca9872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "loss = loss(output, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec0a9149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5476, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11a22d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = output.view(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43f02031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a076db77",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.FloatTensor([[0.6792],[0.6781]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38e4b3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01b428da",
   "metadata": {},
   "outputs": [],
   "source": [
    "testReshaped = test.view(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0cf4ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6792, 0.6781])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testReshaped"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
