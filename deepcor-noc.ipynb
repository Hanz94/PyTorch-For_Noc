{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f81a97d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e00f34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"/home/hansika/gem5/gem5/scripts/numpy_data_test/64_nodes/X/2.npy\",  allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca51c86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76608.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size/900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "249b1ec2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e357abe7fdbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.count_nonzero(data == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1e391264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir, file_index,):\n",
    "        self.data_dir = data_dir\n",
    "        self.file_index = file_index\n",
    "        y = np.load(self.data_dir + \"Y/\" + str(self.file_index) + \".npy\", allow_pickle=True)\n",
    "        self.y = torch.from_numpy(y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = np.load(self.data_dir + \"X/\" + str(self.file_index) + \".npy\", allow_pickle=True, mmap_mode='r')\n",
    "        x = torch.from_numpy(x)\n",
    "        return [x[index], self.y[index]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "27ae120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MyDataset(\"/home/hansika/gem5/gem5/scripts/numpy_data/64_nodes/\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e936956b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399168"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d6e464fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  7,  50,  52,   1,   1,   1,   2,  31,  94,  60,   1,   1,   1,   2,\n",
       "           4,   1,   1,   1,   2, 176, 156,  29,  44, 190,  21,   1,   1,   1,\n",
       "           2,  10,   1,   1,   1,   2,  58,  57,  35,  24,   1,   1,   1,   2,\n",
       "          27,   1,   1,   1,   2,  69,   1,   1,   1,   2,   3,   1,   1,   1,\n",
       "           2,  22,  42,   1,   1,   1,   2,  68,  77,   8,   1,   1,   1,   2,\n",
       "         172,   1,   1,   1,   2,  60,   1,   1,   1,   2,  35,   1,   1,   1,\n",
       "           2,  19,   1,   1,   1,   2, 140,  78, 151, 333, 102,   1,   1,   1,\n",
       "           1,   1,  66,  22,   1,   1,   1,   2,  38,  34, 123,   1,   1,   1,\n",
       "           2,  26, 196,  76,   1,   1,   1,   2,   1,   1,   1,   1,   1,  68,\n",
       "           1,   1,   1,   2,  54,   1,   1,   1,   2,  80,   1,   1,   1,   2,\n",
       "          48,   1,   1,   1,   2,   1,   7, 127,  64,   1,   1,   1,   2, 134,\n",
       "           1,   1,   1,   2,  46,   1,   1,   1,   2,  34, 140,   1,   1,   1,\n",
       "           2,  19,  92,   3, 127,  65, 180,   1,   1,   1,   2,   6,  90,   1,\n",
       "           1,   1,   2,  24, 343,  90,   1,   1,   1,   2,  28, 160, 209, 132,\n",
       "         140,  15,   1,   1,   1,   2, 144,   1,   1,   1,   1,   1,  39,  74,\n",
       "          65,  28, 209, 126, 266,   3,  92, 196,   1,   1,   1,   2, 137,   1,\n",
       "           1,   1,   2,  30,  68,  94,   1,   1,   1,   2,  87,  55, 112, 263,\n",
       "          25, 168,  89,   1,   1,   1,   2, 242, 241,   1,   1,   1,   2,  33,\n",
       "           1,   1,   1,   2,   4,  38,   7,   1,   1,   1,   2,  29,   1,   1,\n",
       "           1,   2,  28,   1,   1,   1,   2, 179,  81,  31,  10,  11,   1,   1,\n",
       "           1,   3,  93,  66,   1,   1,   1,   2,  98,   1,   1,   1,   2,  51,\n",
       "           1,   1,   1,   2, 143,   1,   1,   1,   2,  81,  42,  27,   1,   1,\n",
       "           1,   2,  55,  94,   1,   1,   1,   2, 179,   1,   1,   1,   2,   4,\n",
       "           1,   1,   1,   2, 174,   1,   1,   1,   2,  57, 194, 248,  72,   1,\n",
       "           1,   1,   2,  84,  71, 206,  38, 201,   1,   1,   1,   2,  46, 321,\n",
       "          59,  28,   1,   1,   1,   2,   9, 157,  66,   2,   1,   1,   1,   2,\n",
       "         191,  41,   1,   1,   1,   1,   1,  96,   1,   1,   1,   2, 186,   1,\n",
       "           1,   1,   2, 204,  96,   1,   1,   1,   2,  58, 679, 153,   1,   1,\n",
       "           1,   2, 309,  87,   1,   1,   1,   2,  55,   1,   1,   1,   2, 163,\n",
       "          20, 144,   1,   1,   1,   1,   1,   1,   1,   1,   1,  33,  44,  54,\n",
       "         112,   2, 194,  61,  13,   1,   1,   1,   2,  76,  68,   1,   1,   1,\n",
       "           2,  82,   3, 423, 156,  69, 136, 106,  23,  17,   1,   1,   1,   2,\n",
       "          16,  66],\n",
       "        [ 20, 192,   2,   1,   1,   1,  10, 230, 315,  66,   3,   2,   1,   1,\n",
       "          25,  10, 151, 250,  10, 298,   1,   1,   1,   2,   5, 426,  19,   1,\n",
       "           1,   1,   2,   5,   1,   1,   1,   2, 209,   1,   1,   1,   2,  40,\n",
       "          40,  38, 154,  82,  53,  41, 126,  76, 346,  49,   2,   1,   1,   1,\n",
       "         203,   1,   1,   1,   2,   4, 498,   1,   1,   2,   2, 201,  45,   1,\n",
       "           1,   1,   2,  84,   1,   1,   1,   2, 195,   1,   1,   1,   2,  11,\n",
       "           1,   1,   1,   2, 125,  97,   1,   2,   1,   1,  44, 371,   1,   1,\n",
       "           1,   2,  17,  14,  74, 196,   1,   1,   2,   1, 146,  78,   1,   1,\n",
       "           1,   2,  20,  82, 212,  19,  65,   1,   1,   1,   2, 142,   1,   1,\n",
       "           1,   2,  17, 100,   1,   1,   1,   2,  58, 904,  82,  54,  60,  34,\n",
       "           1,   1,   1,   2,   1, 214, 515, 166, 165,   1,   1,   1,   2, 132,\n",
       "           1,   1,   1,   2,  24,  18,  55,   1,   1,   1,   2,  83,   1,   1,\n",
       "           1,   2,  54,  42,  22,  15,   3, 105,   3, 253,   2,   1,   1,   1,\n",
       "         250,  92,   3,   1,   1,   1,   2,   1,   3,   1,   1,   1,   1,   1,\n",
       "         229,  39,   1,   1,   1,   2,   1, 172,  36,   1,   1,   1,   2,  10,\n",
       "          85,   1,   1,   1,   2,  35,  55,   3,  68,   2,   2,   2,   1,   1,\n",
       "           2,  11,  27,   8, 240,  91,   4, 416,  25,   1,   1,   1,   2, 221,\n",
       "         109,   1,   1,   1,   2,  94,   2,   2,   2,   2,  26,  33,  22, 110,\n",
       "          57,   1,   1,   1,   2, 400, 124,  38, 291, 169,   1,   1,   1,   2,\n",
       "          39, 171,  29,  45,   1,   1,   1,   2, 343,  21,   1,   1,   1,   2,\n",
       "          18,   1,   1,   1,   2,  10, 153,   4,   2,   2,   2,  49,   1,   1,\n",
       "           1,   2,  10,  82, 226,   1,   1,   1,   2,  34,  22,  40,   1,   1,\n",
       "           1,   2, 266, 348,  55,  12,  13,  26,  89, 176,   1,  33,   1,   1,\n",
       "           1,   2, 306, 193,   9,  17,  17, 113,   1,   1,   1,   2,  28,  33,\n",
       "         452,  66,   1,   1,   1,   2,  31,  97,  51, 105,   1,   1,   1,   2,\n",
       "          34,   1,   1,   1,   2,  57,  62,   2,   2,   2,   2,  97, 322,   1,\n",
       "           1,   1,   2, 118,  23,   1,   1,   1,   2,   3, 126,   1,   1,   1,\n",
       "           2,  22, 317, 232,   2,   1,   1,   1,  82,  47,   1,   1,   1,   2,\n",
       "          93,  52,  65,   1,   1,   1,   2,  57,   1,   1,   1,   2, 102,  85,\n",
       "         151, 125, 131,   6,  59, 144,  75,   2,   1,   1,   1,  26,   1,   1,\n",
       "           1,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1c29fff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dataset = []\n",
    "number_of_files = 41\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "for i in range(number_of_files):\n",
    "    list_of_dataset.append(MyDataset(\"/home/hansika/gem5/gem5/scripts/numpy_data/64_nodes/\",i))\n",
    "\n",
    "full_dataset = ConcatDataset(list_of_dataset)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ba890116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16257024"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "64790e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 2000, kernel_size=(2, 30), stride=(2, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=(1, 5), stride=(1, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(2000, 1000, kernel_size=(1, 10), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=(1, 5), stride=(1, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=454000, out_features=3000, bias=True)\n",
      "  (fc2): Linear(in_features=3000, out_features=800, bias=True)\n",
      "  (fc3): Linear(in_features=800, out_features=100, bias=True)\n",
      "  (fc4): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hansika/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448216815/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# W1, W2, K1, K2 are hyper parameters that eventually needed training\n",
    "W1 = 30\n",
    "W2 = 10\n",
    "K1= 2000\n",
    "K2 = 1000\n",
    "\n",
    "#dummy data to try the NN ( 2 arrays of size 500)\n",
    "dummy = torch.randn(2,500).view(-1,1,2,500)\n",
    "\n",
    "# represents the whole CNN\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.conv1 = nn.Conv2d(1, K1, (2,W1), stride=(2, 1))\n",
    "        self.pool1 = nn.MaxPool2d((1, 5), stride=(1, 1))\n",
    "        self.conv2 = nn.Conv2d(K1, K2, (1,W2), stride=(1, 1))\n",
    "        self.pool2 = nn.MaxPool2d((1, 5), stride=(1, 1))\n",
    "        \n",
    "        self.fc1 = nn.Linear(1000*454, 3000) # need to automate arriving at this number (1000*254)\n",
    "        self.fc2 = nn.Linear(3000, 800) \n",
    "        self.fc3 = nn.Linear(800,100)\n",
    "        self.fc4 = nn.Linear(100,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        \n",
    "        x = x.view(-1, 1000*454)    \n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return torch.sigmoid(x)\n",
    "        \n",
    "\n",
    "    \n",
    "## ------------------------------------testing small net--------------------------- ##\n",
    "\n",
    "# net = SmallNet()\n",
    "# print(net)\n",
    "# print(type(dummy))\n",
    "\n",
    "# output = net.forward(dummy.narrow(2,0,2))\n",
    "# output.shape\n",
    "\n",
    "## --------------------------------end testing small net----------------------------- ##\n",
    "\n",
    "## ---------------------------------------- testing net ----------------------------- ##\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "output = net.forward(dummy)\n",
    "output.shape\n",
    "\n",
    "## -------------------------------end testing net------------------------------------ ##\n",
    "\n",
    "\n",
    "\n",
    "# ------------------- Training the CNN ------------------------------------- ##\n",
    "# For now this code is only to show the structure, I need to add data preparation and modify code accordingly.\n",
    "\n",
    "isTraining = False\n",
    "if isTraining:\n",
    "    \n",
    "    # need to use collected data here\n",
    "    trainset = []\n",
    "    testset = []\n",
    "\n",
    "\n",
    "    BATCH_SIZE = 8\n",
    "    EPOCHS = 3\n",
    "\n",
    "    # learning rate of the adam optimizer should be a hyperparameter\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        for data in trainset:\n",
    "            X, y = data \n",
    "            net.zero_grad()  \n",
    "            output = net(X.view(-1,784))  \n",
    "            loss = nn.CrossEntropyLoss()\n",
    "            loss = loss(input, y)\n",
    "            loss.backward() \n",
    "            optimizer.step()  \n",
    "        print(loss)  \n",
    "\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testset:\n",
    "            X, y = data\n",
    "            output = net(X.view(-1,784))\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "    print(\"Accuracy: \", round(correct/total, 3))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "11a22d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5096]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
